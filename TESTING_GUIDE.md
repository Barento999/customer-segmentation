# ğŸ§ª Testing Guide - Customer Segmentation ML

Complete testing infrastructure for the Customer Segmentation ML application.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Backend Tests (pytest)](#backend-tests-pytest)
- [Frontend Tests (Vitest + React Testing Library)](#frontend-tests-vitest--react-testing-library)
- [E2E Tests (Playwright)](#e2e-tests-playwright)
- [Running All Tests](#running-all-tests)
- [CI/CD Integration](#cicd-integration)

---

## ğŸ¯ Overview

This project includes three types of tests:

1. **Backend Unit Tests** - Test API endpoints, ML model, and schemas
2. **Frontend Component Tests** - Test React components and services
3. **E2E Tests** - Test complete user workflows across the full stack

### Test Coverage

- âœ… API endpoint validation
- âœ… ML model training and prediction
- âœ… Schema validation
- âœ… React component rendering
- âœ… Form validation
- âœ… User interactions
- âœ… Full user workflows
- âœ… Mobile responsiveness
- âœ… API integration

---

## ğŸ Backend Tests (pytest)

### Setup

1. **Install test dependencies:**

```bash
cd backend
pip install -r requirements-dev.txt
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage report
pytest --cov=app --cov-report=html

# Run specific test file
pytest tests/test_api.py

# Run specific test class
pytest tests/test_api.py::TestHealthCheck

# Run specific test
pytest tests/test_api.py::TestHealthCheck::test_health_check

# Run with verbose output
pytest -v

# Run and stop on first failure
pytest -x
```

### Test Structure

```
backend/tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py              # Fixtures and configuration
â”œâ”€â”€ test_api.py              # API endpoint tests
â”œâ”€â”€ test_model.py            # ML model tests
â””â”€â”€ test_schema.py           # Schema validation tests
```

### What's Tested

#### API Endpoints (`test_api.py`)

- âœ… Health check endpoint
- âœ… Model training endpoint
- âœ… Prediction endpoint with valid/invalid data
- âœ… Cluster statistics endpoint
- âœ… Elbow method endpoint
- âœ… Input validation
- âœ… Error handling

#### ML Model (`test_model.py`)

- âœ… Model initialization
- âœ… Training with optimal clusters
- âœ… Training with specific cluster count
- âœ… Predictions after training
- âœ… Cluster statistics calculation
- âœ… Elbow method data generation
- âœ… Error handling for untrained model

#### Schemas (`test_schema.py`)

- âœ… CustomerInput validation
- âœ… Age range validation (18-100)
- âœ… Income validation (â‰¥ 0)
- âœ… Spending score validation (1-100)
- âœ… Purchase frequency validation (â‰¥ 0)
- âœ… Required field validation
- âœ… Response schema validation

### Coverage Report

After running tests with coverage, open the HTML report:

```bash
# Windows
start htmlcov/index.html

# Mac/Linux
open htmlcov/index.html
```

---

## âš›ï¸ Frontend Tests (Vitest + React Testing Library)

### Setup

1. **Install test dependencies:**

```bash
cd frontend
npm install
```

### Running Tests

```bash
# Run all tests
npm test

# Run tests in watch mode
npm test -- --watch

# Run with UI
npm run test:ui

# Run with coverage
npm run test:coverage

# Run specific test file
npm test -- CustomerForm.test.jsx

# Run tests matching pattern
npm test -- --grep "validation"
```

### Test Structure

```
frontend/src/tests/
â”œâ”€â”€ setup.js                          # Test configuration
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ CustomerForm.test.jsx         # Form component tests
â”‚   â””â”€â”€ Navbar.test.jsx               # Navigation tests
â””â”€â”€ services/
    â””â”€â”€ api.test.js                   # API service tests
```

### What's Tested

#### CustomerForm Component

- âœ… Renders all form fields
- âœ… Validates empty fields
- âœ… Validates age range (18-100)
- âœ… Validates spending score range (1-100)
- âœ… Validates negative values
- âœ… Submits valid data
- âœ… Clears errors on input
- âœ… Shows loading state
- âœ… Disables button when loading
- âœ… Sex dropdown selection

#### Navbar Component

- âœ… Renders logo and brand
- âœ… Renders all navigation links
- âœ… Correct href attributes
- âœ… Mobile menu button

#### API Service

- âœ… Health check call
- âœ… Train model call
- âœ… Predict segment call
- âœ… Get clusters call
- âœ… Get elbow data call
- âœ… Error handling

### Coverage Report

After running tests with coverage:

```bash
# Windows
start coverage/index.html

# Mac/Linux
open coverage/index.html
```

---

## ğŸ­ E2E Tests (Playwright)

### Setup

1. **Install Playwright:**

```bash
cd e2e
npm install
npx playwright install
```

### Running Tests

```bash
# Run all tests (headless)
npm test

# Run tests with browser visible
npm run test:headed

# Run tests with UI mode
npm run test:ui

# Run tests in debug mode
npm run test:debug

# Run specific test file
npx playwright test home.spec.js

# Run tests on specific browser
npx playwright test --project=chromium

# Run tests on mobile
npx playwright test --project="Mobile Chrome"

# View test report
npm run report

# Generate test code
npm run codegen
```

### Test Structure

```
e2e/tests/
â”œâ”€â”€ home.spec.js              # Home page tests
â”œâ”€â”€ dashboard.spec.js         # Dashboard tests
â”œâ”€â”€ full-workflow.spec.js     # Complete user workflows
â””â”€â”€ api.spec.js               # API integration tests
```

### What's Tested

#### Home Page (`home.spec.js`)

- âœ… Page loads correctly
- âœ… Form validation errors
- âœ… Age range validation
- âœ… Spending score validation
- âœ… Form submission
- âœ… Navigation between pages

#### Dashboard (`dashboard.spec.js`)

- âœ… Dashboard page loads
- âœ… Model training
- âœ… Cluster statistics display
- âœ… API status indicator

#### Full Workflow (`full-workflow.spec.js`)

- âœ… Complete workflow: train â†’ predict â†’ view history
- âœ… Navigate through all pages
- âœ… Mobile navigation
- âœ… Form validation scenarios

#### API Integration (`api.spec.js`)

- âœ… Health check endpoint
- âœ… Train model endpoint
- âœ… Predict endpoint with valid data
- âœ… Predict endpoint rejects invalid data
- âœ… Clusters endpoint
- âœ… Elbow endpoint
- âœ… CORS headers

### Test Reports

After running tests, view the HTML report:

```bash
npm run report
```

### Browsers Tested

- âœ… Desktop Chrome
- âœ… Desktop Firefox
- âœ… Desktop Safari
- âœ… Mobile Chrome (Pixel 5)
- âœ… Mobile Safari (iPhone 12)

---

## ğŸš€ Running All Tests

### Quick Test Script

Create a script to run all tests:

**Windows (`run-all-tests.bat`):**

```batch
@echo off
echo ========================================
echo Running Backend Tests
echo ========================================
cd backend
call venv\Scripts\activate
pytest --cov=app --cov-report=term-missing
cd ..

echo.
echo ========================================
echo Running Frontend Tests
echo ========================================
cd frontend
call npm test -- --run
cd ..

echo.
echo ========================================
echo Running E2E Tests
echo ========================================
cd e2e
call npm test
cd ..

echo.
echo ========================================
echo All Tests Complete!
echo ========================================
```

**Mac/Linux (`run-all-tests.sh`):**

```bash
#!/bin/bash

echo "========================================"
echo "Running Backend Tests"
echo "========================================"
cd backend
source venv/bin/activate
pytest --cov=app --cov-report=term-missing
cd ..

echo ""
echo "========================================"
echo "Running Frontend Tests"
echo "========================================"
cd frontend
npm test -- --run
cd ..

echo ""
echo "========================================"
echo "Running E2E Tests"
echo "========================================"
cd e2e
npm test
cd ..

echo ""
echo "========================================"
echo "All Tests Complete!"
echo "========================================"
```

---

## ğŸ”„ CI/CD Integration

### GitHub Actions Example

Create `.github/workflows/test.yml`:

```yaml
name: Tests

on: [push, pull_request]

jobs:
  backend-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      - name: Run tests
        run: |
          cd backend
          pytest --cov=app --cov-report=xml
      - name: Upload coverage
        uses: codecov/codecov-action@v3

  frontend-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: "18"
      - name: Install dependencies
        run: |
          cd frontend
          npm install
      - name: Run tests
        run: |
          cd frontend
          npm run test:coverage

  e2e-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: "18"
      - uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt
          cd ../frontend
          npm install
          cd ../e2e
          npm install
          npx playwright install --with-deps
      - name: Run E2E tests
        run: |
          cd e2e
          npm test
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: playwright-report
          path: e2e/playwright-report/
```

---

## ğŸ“Š Test Statistics

### Backend Tests

- **Total Tests**: 40+
- **Coverage Target**: >80%
- **Test Files**: 3
- **Average Runtime**: ~10 seconds

### Frontend Tests

- **Total Tests**: 25+
- **Coverage Target**: >70%
- **Test Files**: 3
- **Average Runtime**: ~5 seconds

### E2E Tests

- **Total Tests**: 20+
- **Browsers**: 5
- **Test Files**: 4
- **Average Runtime**: ~2 minutes

---

## ğŸ¯ Best Practices

### Writing Tests

1. **Follow AAA Pattern**: Arrange, Act, Assert
2. **One assertion per test** (when possible)
3. **Use descriptive test names**
4. **Mock external dependencies**
5. **Test edge cases**
6. **Keep tests independent**

### Running Tests

1. **Run tests before committing**
2. **Check coverage reports**
3. **Fix failing tests immediately**
4. **Run E2E tests before deployment**
5. **Use CI/CD for automated testing**

---

## ğŸ› Troubleshooting

### Backend Tests

**Issue**: Tests fail with "Model not found"
**Solution**: Tests create temporary models, ensure write permissions

**Issue**: Import errors
**Solution**: Ensure you're in the virtual environment

### Frontend Tests

**Issue**: "Cannot find module" errors
**Solution**: Run `npm install` in frontend directory

**Issue**: Tests timeout
**Solution**: Increase timeout in vitest.config.js

### E2E Tests

**Issue**: Servers not starting
**Solution**: Ensure ports 8000 and 5173 are available

**Issue**: Browser not found
**Solution**: Run `npx playwright install`

**Issue**: Tests fail on CI
**Solution**: Use `--with-deps` flag when installing Playwright

---

## ğŸ“š Resources

- [pytest Documentation](https://docs.pytest.org/)
- [Vitest Documentation](https://vitest.dev/)
- [React Testing Library](https://testing-library.com/react)
- [Playwright Documentation](https://playwright.dev/)

---

## âœ… Checklist

Before deploying:

- [ ] All backend tests pass
- [ ] All frontend tests pass
- [ ] All E2E tests pass
- [ ] Coverage > 80% for backend
- [ ] Coverage > 70% for frontend
- [ ] Tests run in CI/CD
- [ ] No flaky tests
- [ ] Test documentation updated

---

**Happy Testing! ğŸ‰**
